{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Big_Data_Analytics_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUlMfTqdI/5PsDAdk9vfAj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Susheel06/BDA_Assignment_2/blob/main/Big_Data_Analytics_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz7zx9gs7SEk"
      },
      "source": [
        "import os\n",
        "import operator\n",
        "from collections import defaultdict\n",
        "from itertools import combinations, chain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFUSZ0C2YOQ7",
        "outputId": "83480ac7-ffee-4a7e-805f-8a49a4e7b4d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/Susheel06/BDA_Assignment_2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BDA_Assignment_2'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLPqy9VzB0ig"
      },
      "source": [
        "class Apriori:\n",
        "\n",
        "    def __init__(self, minSupport):\n",
        "        self.support_count = defaultdict(int)\n",
        "        self.minSupport = minSupport\n",
        "\n",
        "    def read_transactions_from_file(self, transaction_file):\n",
        "        with open(transaction_file, \"r\") as infile:\n",
        "            transactions = [set(line.rstrip(\"\\n\").split(\";\"))\n",
        "                            for line in infile]\n",
        "\n",
        "            return transactions\n",
        "\n",
        "    def get_one_itemset(self, transactions):\n",
        "        one_itemset = set()\n",
        "        for transaction in transactions:\n",
        "            for item in transaction:\n",
        "                one_itemset.add(frozenset([item]))\n",
        "\n",
        "        return one_itemset\n",
        "\n",
        "    def self_cross(self, Ck, itemset_size):\n",
        "        Ck_plus_1 = {itemset1.union(itemset2)\n",
        "                     for itemset1 in Ck for itemset2 in Ck\n",
        "                     if len(itemset1.union(itemset2)) == itemset_size}\n",
        "        return Ck_plus_1\n",
        "\n",
        "    def prune_Ck(self, Ck, Lk_minus_1, itemset_size):\n",
        "        Ck_ = set()\n",
        "        for itemset in Ck:\n",
        "            Ck_minus_1 = list(combinations(itemset, itemset_size-1))\n",
        "            flag = 0\n",
        "            for subset in Ck_minus_1:\n",
        "                if not frozenset(subset) in Lk_minus_1:\n",
        "                    flag = 1\n",
        "                    break\n",
        "            if flag == 0:\n",
        "                Ck_.add(itemset)\n",
        "        return Ck_\n",
        "\n",
        "    def get_min_supp_itemsets(self, Ck, transactions):\n",
        "        temp_freq = defaultdict(int)\n",
        "\n",
        "        # update support count of each itemset\n",
        "        for transaction in transactions:\n",
        "            for itemset in Ck:\n",
        "                if itemset.issubset(transaction):\n",
        "                    temp_freq[itemset] += 1\n",
        "                    self.support_count[itemset] += 1\n",
        "\n",
        "        N = len(transactions)\n",
        "        Lk = [itemset for itemset, freq in temp_freq.items()\n",
        "              if freq/N > self.minSupport]\n",
        "        return set(Lk)\n",
        "\n",
        "    def frequent_item_set(self, transactions):\n",
        "        K_itemsets = dict()\n",
        "        Ck = self.get_one_itemset(transactions)\n",
        "        Lk = self.get_min_supp_itemsets(Ck, transactions)\n",
        "        k = 2\n",
        "        while len(Lk) != 0:\n",
        "            K_itemsets[k-1] = Lk\n",
        "            Ck = self.self_cross(Lk, k)\n",
        "            Ck = self.prune_Ck(Ck, Lk, k)\n",
        "            Lk = self.get_min_supp_itemsets(Ck, transactions)\n",
        "            k += 1\n",
        "\n",
        "        return K_itemsets\n",
        "\n",
        "    def subsets(self, iterable):\n",
        "        list_ = list(iterable)\n",
        "        subsets_ = chain.from_iterable(combinations(list_, len)\n",
        "                                       for len in range(len(list_)+1))\n",
        "        subsets_ = list(map(frozenset, subsets_))\n",
        "\n",
        "        return subsets_\n",
        "\n",
        "    def write_part_1(self, K_itemsets):\n",
        "        main_dir = \"/content/BDA_Assignment_2\"\n",
        "        if not os.path.exists(main_dir):\n",
        "            os.makedirs(main_dir)\n",
        "\n",
        "        outfile_path = \"/content/BDA_Assignment_2/patterns.txt\"\n",
        "        with open(outfile_path, \"w\") as outfile:\n",
        "            for key, values in K_itemsets.items():\n",
        "                if key > 1:\n",
        "                    break\n",
        "                for value in values:\n",
        "                    support_ct = self.support_count[value]\n",
        "                    outfile.write(\"{support}:{label}\\n\".format(\n",
        "                        support=support_ct,\n",
        "                        label=\";\".join(list(value))\n",
        "                    ))\n",
        "\n",
        "    def write_part_2(self, K_itemsets):\n",
        "        main_dir = '/content/BDA_Assignment_2'\n",
        "        if not os.path.exists(main_dir):\n",
        "            os.makedirs(main_dir)\n",
        "\n",
        "        outfile_path = \"/content/BDA_Assignment_2/patterns.txt\"\n",
        "        with open(outfile_path, \"w\") as outfile:\n",
        "            for key, values in K_itemsets.items():\n",
        "                for value in values:\n",
        "                    support_ct = self.support_count[value]\n",
        "                    outfile.write(\"{support}:{label}\\n\".format(\n",
        "                        support=support_ct,\n",
        "                        label=\";\".join(list(value))\n",
        "                    ))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    in_transaction_file = \"/content/BDA_Assignment_2/categories.txt.txt\"\n",
        "\n",
        "    ap = Apriori(minSupport=0.01)\n",
        "    transactions = ap.read_transactions_from_file(in_transaction_file)\n",
        "    K_itemsets = ap.frequent_item_set(transactions)\n",
        "    ap.write_part_1(K_itemsets)\n",
        "    ap.write_part_2(K_itemsets)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}